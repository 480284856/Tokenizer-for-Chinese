{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:  # 把字理解为附着在边的，节点名字为数字\n",
    "    def __init__(self,preNode : 'Node', allArrows: dict, nodeName : int):\n",
    "        \"\"\"\n",
    "        创建一个字节点。\n",
    "\n",
    "        一个节点应该考虑以下事情：\n",
    "        1.从0节点到该节点的边的权值的和的最优值\n",
    "        2.该节点有多少个相邻结点，就应该比较0节点通过这几个节点到达该节点的边的权值之和中，哪个是最低的。（小数的负对数，越小，越接近1）\n",
    "        \n",
    "        ----\n",
    "        Attributes:\n",
    "            self.name: int\n",
    "                节点的名字\n",
    "            self.preNode: <class, Node>\n",
    "                前一个节点\n",
    "            self.distanceFromZero: int\n",
    "                0节点到该节点的最小距离\n",
    "        ----\n",
    "        Pamameter(s):\n",
    "            preNode: <class, Node>\n",
    "                该节点之前的可以直接到其的节点(最多只有一个)\n",
    "            allArrows: dict of {arrowSerialNumber: <class, Arrow>}\n",
    "                所有的有向边\n",
    "            nodeName:\n",
    "                当前节点的代号\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"通过nodeName，然后遍历所有的边，取出所有目的地为该节点的边\"\"\"\n",
    "        arrows = []  # 记录所有目的地为当前节点的边\n",
    "        for arrow in allArrows.values():\n",
    "            if arrow.endNode == nodeName:  # 该有向边的目的地是当前节点\n",
    "                arrows.append(arrow)\n",
    "\n",
    "        global allNodes\n",
    "\n",
    "        self.name = nodeName\n",
    "        if arrows == []:  # 该节点是第一个节点，也就是说要创建第一个节点\n",
    "            self.distanceFromZero = 0\n",
    "            self.preNode = None  #记录当前节点的前一个节点\n",
    "            self.arrowFrom = None\n",
    "        else:\n",
    "            distances = []\n",
    "            for arrow in arrows:\n",
    "                arrowWeight = arrow.weight  # preNode经过arrow可以到达当前节点\n",
    "                preNodeDistance = allNodes[ arrow.preNode ].distanceFromZero  # 每条边对应的preNode都不一样\n",
    "                distances.append( preNodeDistance+arrowWeight )  # 记录0节点从每条路到当前节点的距离\n",
    "            bestNodeIndex = np.argmin(distances)  # 选出最优距离\n",
    "            self.distanceFromZero = distances[bestNodeIndex]\n",
    "            self.preNode = allNodes[ arrows[bestNodeIndex].preNode ]  # bestNodeIndex对应arrow里面距离最小的，取出代号后得到Node\n",
    "            self.arrowFrom = arrows[bestNodeIndex]\n",
    "\n",
    "\n",
    "class Arrow:  \n",
    "    def __init__(self,arrowName: str, arrowSerNum : int, preNode: int, endNode: int):\n",
    "        \"\"\"\n",
    "        Attribute(s):\n",
    "        self.name: str\n",
    "            边的名字\n",
    "        self.weight: float\n",
    "            边从权重\n",
    "        self.preNode: <class, Node>\n",
    "            该边的出发节点\n",
    "        ----\n",
    "        Pramater(s):\n",
    "            arrowName: \n",
    "                边的名字\n",
    "            arrowSerNum:\n",
    "                边的数字序列号\n",
    "            preNode:\n",
    "                该边的出发节点的数字代号\n",
    "            endNode:\n",
    "                该边到达的节点的数字代号\n",
    "        \"\"\"\n",
    "        global ngram_logValue\n",
    "        try:\n",
    "            self.weight = ngram_logValue[arrowName]  # 该token是unigram\n",
    "        except:\n",
    "            self.weight = ngram_logValue[arrowName]  # 该token是bigram\n",
    "        finally:\n",
    "            self.name = arrowName\n",
    "            self.serialNumber = arrowSerNum   \n",
    "            self.preNode = preNode\n",
    "            self.endNode = endNode\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self,file_ngram: 'file'):\n",
    "        \"\"\"\n",
    "        中文分词\n",
    "        ----\n",
    "        Pamameter(s):\n",
    "        file_ngram:\n",
    "            ngram语料库\n",
    "        \"\"\"\n",
    "\n",
    "        self.file_ngram = file_ngram\n",
    "        self.ngramProbabilities = self.ngramProbability(self.file_ngram)  # 计算概率\n",
    "\n",
    "\n",
    "    def encode(self,sentence : str) -> None:\n",
    "        \"\"\"\n",
    "        最大化概率2-gram中文分词\n",
    "\n",
    "        ----\n",
    "        Parameter(s):\n",
    "        sentence:\n",
    "            输入\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        1. 切割sentence,切成字的unigram和bigram（不是词的unigram和bigram）\n",
    "        2. 计算每个gram出现的概率的负对数        \n",
    "        \"\"\"\n",
    "        self.sentence = sentence\n",
    "\n",
    "        unigram = [x for x in sentence]\n",
    "        offsets = []\n",
    "        for i in range(len(unigram)):\n",
    "            offsets.append((i,i+len(unigram[i])))\n",
    "        ngrams = self.ForwardMaxMatch(windowsLen=5) \n",
    "        i=0\n",
    "        j=0\n",
    "        while(i < len(sentence)):\n",
    "            end = i+len(ngrams[j])\n",
    "            offsets.append( (i,end ) )\n",
    "            i = end \n",
    "            j += 1\n",
    "\n",
    "        # ngrams = list(set(ngrams))  #去掉相同的gram\n",
    "        ngrams = unigram+ngrams\n",
    "        global ngram_logValue\n",
    "        ngram_logValue = self.logProba_unigram(ngrams,self.ngramProbabilities)\n",
    "\n",
    "        graph = self.CreateGraph(ngrams, offsets)  # 创建有向图\n",
    "        \n",
    "        return self.display(graph)\n",
    "\n",
    "\n",
    "    def ForwardMaxMatch(self, windowsLen) -> list:\n",
    "        \"\"\"\n",
    "        前向最大匹配算法\n",
    "        \n",
    "        ----\n",
    "        Parameter(s):\n",
    "            windowsLen:\n",
    "                每次选取的字符串长度\n",
    "        Return:\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        从索引0开始，根据字符串长度选取一段子字符串，然后对这个子字符串做从\n",
    "        右到左扫描，扫描过程中，末指针往前走，只要始末指针划定的字符串在词典中出现了，\n",
    "        那么就把这个字符串作为一个ngram，然后把开始指针设定为末指针+1，继续扫描，\n",
    "        直到初始指针等于句子的长度，表示初始指针已经走出字符串了，所以结束循环。\n",
    "        \"\"\"\n",
    "\n",
    "        sentence = self.sentence\n",
    "        start = 0\n",
    "        result = []\n",
    "        while start < len(sentence):\n",
    "            end = min(len(sentence)-1, start+windowsLen-1)  # 防止越界\n",
    "            for i in range(end,start-1,-1):\n",
    "                subGram = sentence[start:i+1]\n",
    "                if self.isInDict(subGram):\n",
    "                    result.append(subGram)\n",
    "                    start = i+1  #start跳到subGram的下一个字符那里\n",
    "                    break  #匹配完成，进行下一次匹配\n",
    "        return result\n",
    "                \n",
    "\n",
    "    def isInDict(self,subGram) -> bool:\n",
    "        \"\"\"判断某个gram是否在词典中\"\"\"\n",
    "        return subGram in self.ngramProbabilities\n",
    "\n",
    "\n",
    "    def display(self, graph: dict):\n",
    "        Str = []\n",
    "        end = len(graph)-1\n",
    "        node = graph[end]\n",
    "        while 1:\n",
    "            try:\n",
    "                Str.append( node.arrowFrom.name )\n",
    "                node = node.preNode\n",
    "            except:\n",
    "                break\n",
    "        return Str[::-1]\n",
    "\n",
    "\n",
    "    def getNgramSpan(self, ngram: str) -> list:\n",
    "        \"\"\"\n",
    "        得到ngram的出发节点和结束节点\n",
    "        \n",
    "        ----\n",
    "        Return:\n",
    "            [preNode,endNode]: ngram这条边的出发节点和结束节点\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ngram的始末相当于这个ngram在原始句子中的索引的始末，可以通过index方法\n",
    "        得到ngram在句子中的初始位置，这个初始位置刚好对应出发节点（参见实验文档中的示意图），\n",
    "        末位置在加个1，对应到达节点。\n",
    "        \"\"\"\n",
    "        preNode = self.sentence.index(ngram)\n",
    "        endNode = preNode+len(ngram)\n",
    "        return [preNode,endNode]\n",
    "\n",
    "\n",
    "    def CreateGraph(self, ngrams : list, offsets: list) -> dict:\n",
    "        \"\"\"\n",
    "        根据unigram和其他ngram创建有向图\n",
    "        ----\n",
    "        Parameter(s):\n",
    "            ngrams:\n",
    "                所有的gram的集合\n",
    "            offsets:\n",
    "                这些gram当作边时的出发节点和到达节点\n",
    "        Return:\n",
    "            graph:\n",
    "                有向图。\n",
    "        \"\"\"\n",
    "\n",
    "        allArrows = {}\n",
    "        # lenUnigram = len(unigram)\n",
    "        # for i in range(lenUnigram):\n",
    "        #     allArrows[i] = Arrow(unigram[i], i, i, i+1)  # i刚好对应边出发的节点的代号\n",
    "        for i in range(len(ngrams)):\n",
    "            preNode,endNode = offsets[i]\n",
    "            allArrows[i] = Arrow(ngrams[i], i, preNode, endNode)\n",
    "\n",
    "        global allNodes\n",
    "        allNodes = {}\n",
    "        allNodes[0] = Node(None,allArrows,0)\n",
    "        allNodes[1] = Node(allNodes[0],allArrows,1)\n",
    "\n",
    "        for i in range(2, len(self.sentence)+1):\n",
    "            preNodes = allNodes[i-1]\n",
    "            # allArrow： 0-lenUnigram-1是unigram的arrow，后面的则是bigram的arrow\n",
    "            allNodes[i] = Node(preNodes,allArrows,i)\n",
    "            \n",
    "        graph = allNodes        \n",
    "        return graph\n",
    "\n",
    "\n",
    "    def logProba_unigram(self, ngrams : list ,ngramProbabilities: dict) -> dict:\n",
    "        \"\"\"\n",
    "        计算ngrams中的每个gram在语料库中的统计概率\n",
    "\n",
    "        ----\n",
    "        Parameter(s):\n",
    "            ngrams: \n",
    "                FMM产生的ngram的结果\n",
    "            ngramProbabilities:\n",
    "                语料库中所有ngram的概率的负对数\n",
    "        Return:\n",
    "            result:\n",
    "                一个字典，例如{\"我\": 2.1231}\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for gram in ngrams:\n",
    "            try:\n",
    "                probability = ngramProbabilities[gram]\n",
    "            except:\n",
    "                probability = self.getUnknowedProba()  # 语料库中没有这个字，那么使用加法平滑，给出一个小概率的负对数\n",
    "            result[gram] = probability\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def logProba_bigram(self, bigram : list ,bigramProbabilities: dict) -> dict:\n",
    "        \"\"\"\n",
    "        计算bigram中的每个字在语料库中的统计概率\n",
    "\n",
    "        ----\n",
    "        Parameter(s):\n",
    "            bigram: \n",
    "                单个字的gram\n",
    "            bigramProbabilities:\n",
    "                语料库中所有unigram的概率的负对数\n",
    "        Return:\n",
    "            result:\n",
    "                一个字典，例如{\"我\": 2.1231}\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for gram in bigram:\n",
    "            try:\n",
    "                probability = bigramProbabilities[gram]\n",
    "            except:\n",
    "                probability = self.getUnknowedProba_bigram()  # 语料库中没有这个字，那么使用加法平滑，给出一个小概率的负对数\n",
    "            result[gram] = probability\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def getUnknowedProba(self) -> int:\n",
    "        \"\"\"\n",
    "        使用加法平滑，给出一个未知词的概率的负对数\n",
    "\n",
    "        ----\n",
    "        Return:\n",
    "            proLog:\n",
    "                概率的负对数\n",
    "        \"\"\"\n",
    "        probability = 1/(len(self.ngramProbabilities) + len(self.ngramProbabilities))\n",
    "        return -np.log(probability)\n",
    "\n",
    "\n",
    "    def getUnknowedProba_bigram(self) -> int:\n",
    "        \"\"\"\n",
    "        使用加法平滑，给出一个未知词的概率的负对数\n",
    "\n",
    "        ----\n",
    "        Return:\n",
    "            proLog:\n",
    "                概率的负对数\n",
    "        \"\"\"\n",
    "        probability = 1/(len(self.bigramProbabilities) + len(self.bigramProbabilities))\n",
    "        return -np.log(probability)\n",
    "\n",
    "\n",
    "    def ngramProbability(self,file_ngram) -> dict:\n",
    "        \"\"\"\n",
    "        计算unigram的-log概率\n",
    "        ----\n",
    "        Pamater(s):\n",
    "            file_ngram: file\n",
    "                ngram语料库\n",
    "        Return:\n",
    "            myDict: dict\n",
    "                每个unigram对应的-log对数\n",
    "        \"\"\"\n",
    "        with open(file_ngram, encoding='utf-8') as F:\n",
    "            myDict = {}\n",
    "            allLines = F.readlines()\n",
    "            total_words = 0\n",
    "            for line in allLines:\n",
    "                line = line.split()\n",
    "                total_words += int(line[1])\n",
    "\n",
    "            for line in allLines:\n",
    "                line = line.split()\n",
    "                myDict[line[0]] = -np.log(int(line[1])/total_words)\n",
    "\n",
    "        return myDict\n",
    "\n",
    "\n",
    "    def bigramProbability(self,file_bigram ) -> dict:\n",
    "        \"\"\"\n",
    "        计算bigram的-log概率\n",
    "        ----\n",
    "        Pamater(s):\n",
    "            file_bigram: file\n",
    "                bigram语料库\n",
    "        Return:\n",
    "            myDict: dict\n",
    "                每个bigram对应的-log对数\n",
    "        \"\"\"   \n",
    "        with open(file_bigram, 'r') as F:\n",
    "            myDict = {}\n",
    "            allLines = F.readlines()\n",
    "            total_words = 0\n",
    "            for line in allLines:\n",
    "                line = line.split()\n",
    "                total_words += int(line[1])\n",
    "\n",
    "            for line in allLines:\n",
    "                line = line.split()\n",
    "                myDict[line[0]] = -np.log(int(line[1])/total_words)   \n",
    "\n",
    "        return myDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    with open(\"人民日报96年语料.124-gram.word\", 'x', encoding='utf-8') as newF:\n",
    "        for i in [1,2,4]:\n",
    "            print(i)\n",
    "            with open(\"人民日报96年语料.{}-gram.word\".format(i), encoding='utf-8') as F:\n",
    "                for line in F:\n",
    "                    if line != None:\n",
    "                        newF.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我是', '顾小', '杰', '我', '今', '年二', '十岁', '我在', '巴基斯坦', '卖', '包子']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"我是顾小杰我今年二十岁我在巴基斯坦卖包子\"\n",
    "file_ngram = \"人民日报96年语料.124-gram.word\"\n",
    "Tokenizer(file_ngram).encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '是', '顾', '小', '杰', '我', '在', '巴基斯坦', '卖', '包子']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"我是顾小杰我在巴基斯坦卖包子\"\n",
    "file_ngram = \"人民日报96年语料.unigram\"\n",
    "Tokenizer(file_ngram).encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:  # 把字理解为附着在边的，节点名字为数字\n",
    "    def __init__(self,preNode : 'Node', arrows: list, nodeName : int):\n",
    "        \"\"\"\n",
    "        创建一个字节点。\n",
    "\n",
    "        一个节点应该考虑以下事情：\n",
    "        1.从0节点到该节点的边的权值的和的最优值\n",
    "        2.该节点有多少个相邻结点，就应该比较0节点通过这几个节点到达该节点的边的权值之和中，哪个是最低的。（小数的负对数，越小，越接近1）\n",
    "        \n",
    "        ----\n",
    "        Attributes:\n",
    "            self.name: int\n",
    "                节点的名字\n",
    "            self.preNode: <class, Node>\n",
    "                前一个节点\n",
    "            self.distanceFromZero: int\n",
    "                0节点到该节点的最小距离\n",
    "        ----\n",
    "        Pamameter(s):\n",
    "            preNode: <class, Node>\n",
    "                该节点之前的可以直接到其的节点(最多只有一个)\n",
    "            arrows: list of <class, Arrow>\n",
    "                里面的元素与preNode的元素一一对应，表示preNode通过arrow可以到达当前Node\n",
    "            nodeName:\n",
    "                当前节点的代号\n",
    "        \"\"\"\n",
    "\n",
    "        global allNodes\n",
    "\n",
    "        self.name = nodeName\n",
    "        if preNode == None:  # 该节点是第一个节点，也就是说要创建第一个节点\n",
    "            self.distanceFromZero = 0\n",
    "            self.preNode = None  #记录当前节点的前一个节点\n",
    "            self.arrowFrom = None\n",
    "        else:\n",
    "            distances = []\n",
    "            for arrow in arrows:\n",
    "                arrowWeight = arrow.weight  # preNode经过arrow可以到达当前节点\n",
    "                preNodeDistance = allNodes[ arrow.preNode ].distanceFromZero  # 每条边对应的preNode都不一样\n",
    "                distances.append( preNodeDistance+arrowWeight )  # 记录0节点从每条路到当前节点的距离\n",
    "            bestNodeIndex = np.argmin(distances)  # 选出最优距离\n",
    "            self.distanceFromZero = distances[bestNodeIndex]\n",
    "            self.preNode = allNodes[ arrows[bestNodeIndex].preNode ]  # bestNodeIndex对应arrow里面距离最小的，取出代号后得到Node\n",
    "            self.arrowFrom = arrows[bestNodeIndex]\n",
    "\n",
    "\n",
    "class Arrow:  \n",
    "    def __init__(self,arrowName: str, arrowSerNum : int, preNode: int):\n",
    "        \"\"\"\n",
    "        Attribute(s):\n",
    "        self.name: str\n",
    "            边的名字\n",
    "        self.weight: float\n",
    "            边从权重\n",
    "        self.preNode: <class, Node>\n",
    "            该边的出发节点\n",
    "        ----\n",
    "        Pramater(s):\n",
    "            arrowName: \n",
    "                边的名字\n",
    "            arrowSerNum:\n",
    "                边的数字序列号\n",
    "            preNode:\n",
    "                该边的出发节点的数字代号\n",
    "        \"\"\"\n",
    "        global unigram_logValue,bigram_logValue\n",
    "        try:\n",
    "            self.weight = unigram_logValue[arrowName]  # 该token是unigram\n",
    "        except:\n",
    "            self.weight = bigram_logValue[arrowName]  # 该token是bigram\n",
    "        finally:\n",
    "            self.name = arrowName\n",
    "            self.serialNumber = arrowSerNum   \n",
    "            self.preNode = preNode\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self,file_unigram: 'file', file_bigram : 'file'):\n",
    "        \"\"\"\n",
    "        中文分词\n",
    "        ----\n",
    "        Pamameter(s):\n",
    "        file_unigram:\n",
    "            unigram语料库\n",
    "        file_bigram:\n",
    "            bigram语料库\n",
    "        \"\"\"\n",
    "\n",
    "        self.file_unigram = file_unigram\n",
    "        self.file_bigram = file_bigram\n",
    "        \n",
    "        self.unigramProbabilities = self.unigramProbability(self.file_unigram)  # 计算概率\n",
    "        self.bigramProbabilities = self.bigramProbability(self.file_bigram)  # 计算概率\n",
    "\n",
    "\n",
    "    def encode(self,sentence : str) -> None:\n",
    "        \"\"\"\n",
    "        最大化概率2-gram中文分词\n",
    "\n",
    "        ----\n",
    "        Parameter(s):\n",
    "        sentence:\n",
    "            输入\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        1. 切割sentence,切成字的unigram和bigram（不是词的unigram和bigram）\n",
    "        2. 计算每个gram出现的概率的负对数        \n",
    "        \"\"\"\n",
    "        unigram = [x for x in sentence]\n",
    "        bigram = [sentence[i:i+2] for i in range(len(sentence)-1)]\n",
    "\n",
    "        global unigram_logValue,bigram_logValue\n",
    "        unigram_logValue = self.logProba_unigram(unigram,self.unigramProbabilities)\n",
    "        bigram_logValue = self.logProba_bigram(bigram,self.bigramProbabilities)\n",
    "\n",
    "        graph = self.CreateGraph(unigram, bigram)  # 创建有向图\n",
    "        \n",
    "        return self.display(graph)\n",
    "\n",
    "\n",
    "    def display(self, graph: dict):\n",
    "        Str = []\n",
    "        end = len(graph)-1\n",
    "        node = graph[end]\n",
    "        while 1:\n",
    "            try:\n",
    "                Str.append( node.arrowFrom.name )\n",
    "                node = node.preNode\n",
    "            except:\n",
    "                break\n",
    "        return Str[::-1]\n",
    "\n",
    "\n",
    "    def CreateGraph(self, unigram : list, bigram : list) -> dict:\n",
    "        \"\"\"\n",
    "        根据unigram和bigram创建有向图\n",
    "        ----\n",
    "        Return:\n",
    "            graph:\n",
    "                有向图。\n",
    "        \"\"\"\n",
    "\n",
    "        allArrows = {}\n",
    "        lenUnigram = len(unigram)\n",
    "        for i in range(lenUnigram):\n",
    "            allArrows[i] = Arrow(unigram[i], i, i)  # i刚好对应边出发的节点的代号\n",
    "        for i in range(len(bigram)):\n",
    "            allArrows[i+lenUnigram] = Arrow(bigram[i], i+lenUnigram, i)\n",
    "\n",
    "        global allNodes\n",
    "        allNodes = {}\n",
    "        allNodes[0] = Node(None,[],0)\n",
    "        allNodes[1] = Node(allNodes[0],[allArrows[0]],1)\n",
    "\n",
    "        for i in range(2, len(unigram)+1):\n",
    "            preNodes = allNodes[i-1]\n",
    "            arrows = [ allArrows[ i-1 ], allArrows[ i-2+lenUnigram ] ]  # 0后面的节点都有两条边考虑\n",
    "            # allArrow： 0-lenUnigram-1是unigram的arrow，后面的则是bigram的arrow\n",
    "            allNodes[i] = Node(preNodes,arrows,i)\n",
    "            \n",
    "        graph = allNodes        \n",
    "        return graph\n",
    "\n",
    "\n",
    "    def logProba_unigram(self, unigram : list ,unigramProbabilities: dict) -> dict:\n",
    "        \"\"\"\n",
    "        计算unigram中的每个字在语料库中的统计概率\n",
    "\n",
    "        ----\n",
    "        Parameter(s):\n",
    "            unigram: \n",
    "                单个字的gram\n",
    "            unigramProbabilities:\n",
    "                语料库中所有unigram的概率的负对数\n",
    "        Return:\n",
    "            result:\n",
    "                一个字典，例如{\"我\": 2.1231}\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for gram in unigram:\n",
    "            try:\n",
    "                probability = unigramProbabilities[gram]\n",
    "            except:\n",
    "                probability = self.getUnknowedProba_unigram()  # 语料库中没有这个字，那么使用加法平滑，给出一个小概率的负对数\n",
    "            result[gram] = probability\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def logProba_bigram(self, bigram : list ,bigramProbabilities: dict) -> dict:\n",
    "        \"\"\"\n",
    "        计算bigram中的每个字在语料库中的统计概率\n",
    "\n",
    "        ----\n",
    "        Parameter(s):\n",
    "            bigram: \n",
    "                单个字的gram\n",
    "            bigramProbabilities:\n",
    "                语料库中所有unigram的概率的负对数\n",
    "        Return:\n",
    "            result:\n",
    "                一个字典，例如{\"我\": 2.1231}\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for gram in bigram:\n",
    "            try:\n",
    "                probability = bigramProbabilities[gram]\n",
    "            except:\n",
    "                probability = self.getUnknowedProba_bigram()  # 语料库中没有这个字，那么使用加法平滑，给出一个小概率的负对数\n",
    "            result[gram] = probability\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def getUnknowedProba_unigram(self) -> int:\n",
    "        \"\"\"\n",
    "        使用加法平滑，给出一个未知词的概率的负对数\n",
    "\n",
    "        ----\n",
    "        Return:\n",
    "            proLog:\n",
    "                概率的负对数\n",
    "        \"\"\"\n",
    "        probability = 1/(len(self.unigramProbabilities) + len(self.unigramProbabilities))\n",
    "        return -np.log(probability)\n",
    "\n",
    "\n",
    "    def getUnknowedProba_bigram(self) -> int:\n",
    "        \"\"\"\n",
    "        使用加法平滑，给出一个未知词的概率的负对数\n",
    "\n",
    "        ----\n",
    "        Return:\n",
    "            proLog:\n",
    "                概率的负对数\n",
    "        \"\"\"\n",
    "        probability = 1/(len(self.bigramProbabilities) + len(self.bigramProbabilities))\n",
    "        return -np.log(probability)\n",
    "\n",
    "\n",
    "    def unigramProbability(self,file_unigram) -> dict:\n",
    "        \"\"\"\n",
    "        计算unigram的-log概率\n",
    "        ----\n",
    "        Pamater(s):\n",
    "            file_unigram: file\n",
    "                unigram语料库\n",
    "        Return:\n",
    "            myDict: dict\n",
    "                每个unigram对应的-log对数\n",
    "        \"\"\"\n",
    "        with open(file_unigram, encoding='utf-8') as F:\n",
    "            myDict = {}\n",
    "            allLines = F.readlines()\n",
    "            total_words = 0\n",
    "            for line in allLines:\n",
    "                line = line.split()\n",
    "                total_words += int(line[1])\n",
    "\n",
    "            for line in allLines:\n",
    "                line = line.split()\n",
    "                myDict[line[0]] = -np.log(int(line[1])/total_words)\n",
    "\n",
    "        return myDict\n",
    "\n",
    "\n",
    "    def bigramProbability(self,file_bigram ) -> dict:\n",
    "        \"\"\"\n",
    "        计算bigram的-log概率\n",
    "        ----\n",
    "        Pamater(s):\n",
    "            file_bigram: file\n",
    "                bigram语料库\n",
    "        Return:\n",
    "            myDict: dict\n",
    "                每个bigram对应的-log对数\n",
    "        \"\"\"   \n",
    "        with open(file_bigram, encoding='utf-8') as F:\n",
    "            myDict = {}\n",
    "            allLines = F.readlines()\n",
    "            total_words = 0\n",
    "            for line in allLines:\n",
    "                line = line.split()\n",
    "                total_words += int(line[1])\n",
    "\n",
    "            for line in allLines:\n",
    "                line = line.split()\n",
    "                myDict[line[0]] = -np.log(int(line[1])/total_words)   \n",
    "\n",
    "        return myDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '在', '巴基', '斯坦', '卖包', '子']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"我在巴基斯坦卖包子\"\n",
    "file_unigram = \"人民日报96年语料.unigram.word\"\n",
    "file_bigram = \"人民日报96年语料.bigram.word\"\n",
    "Tokenizer(file_unigram,file_bigram).encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b19d6cbc73657de609bd1dcd16dcefbf34bfd66a06fadf3aa8dcd3e0740b7d77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
